# OptimizaciÃ³n de carteras con aprendizaje por refuerzo profundo (DRL)

Este repositorio implementa un sistema de gestiÃ³n de carteras mediante aprendizaje por refuerzo profundo. Utiliza un entorno compatible con Gym y se apoya en el algoritmo SAC para asignar dinÃ¡micamente pesos entre SPY, TLT, GLD y efectivo.

## CÃ³mo ejecutar el proyecto

Los principales pasos de trabajo se encapsulan en scripts dentro de la carpeta `scripts/`:

### `01_download_data.py`
Descarga los precios histÃ³ricos de los activos y los indicadores macroeconÃ³micos necesarios.

```bash
python scripts/01_download_data.py
```

### `02_explore_data.py`
Genera las visualizaciones utilizadas en la memoria del proyecto a partir del archivo `datasets/combined_data.csv`.

```bash
python scripts/02_explore_data.py
```

### `03_test_env.py`
Crea el entorno de inversiÃ³n y ejecuta dos polÃ­ticas baseline predefinidas (pesos iguales y SPY Ãºnicamente) para comprobar que el entorno funciona correctamente.

```bash
python scripts/03_test_env.py
```

### `04_train_agent.py`
Entrena un agente con SAC. El script permite ajustar, entre otros parÃ¡metros, los rangos de entrenamiento y validaciÃ³n, la semilla, la frecuencia de evaluaciÃ³n y los pasos totales.

```bash
python scripts/04_train_agent.py
```

### `05_evaluate_agent.py`
Carga un modelo entrenado y lo evalÃºa frente a las estrategias clÃ¡sicas implementadas en `src/benchmarks.py`.

```bash
python scripts/05_evaluate_agent.py
```

## ğŸ“ Estructura de carpetas

```
drl_portfolio/
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ 01_download_data.py
â”‚   â”œâ”€â”€ 02_explore_data.py
â”‚   â”œâ”€â”€ 03_test_env.py
â”‚   â”œâ”€â”€ 04_train_agent.py
â”‚   â””â”€â”€ 05_evaluate_agent.py
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data.py                # Carga y preparaciÃ³n de datasets
â”‚   â”œâ”€â”€ observation_builder.py # ConstrucciÃ³n del estado observable
â”‚   â”œâ”€â”€ portfolio_env.py       # Entorno personalizado de inversiÃ³n
â”‚   â”œâ”€â”€ metrics.py             # MÃ©tricas de rendimiento
â”‚   â”œâ”€â”€ benchmarks.py          # Estrategias de referencia
â”‚   â”œâ”€â”€ train.py               # LÃ³gica de entrenamiento y evaluaciÃ³n
â”‚   â””â”€â”€ visualizations.py      # GeneraciÃ³n de grÃ¡ficos de mÃ©tricas
â”‚
â”œâ”€â”€ models/                    # Modelos entrenados
â”œâ”€â”€ datasets/                  # Datos brutos y procesados
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ explore_data/          # GrÃ¡ficos del script 02_explore_data
â”‚   â”œâ”€â”€ model_training/        # MÃ©tricas y logs de entrenamiento
â”‚   â”œâ”€â”€ model_evaluation/      # Resultados de evaluaciÃ³n del agente
â”‚   â””â”€â”€ visualizations/        # Comparativas generadas por visualizations.py
```

## Estado del proyecto

* Entorno Gym funcional y polÃ­ticas baseline incluidas.
* Entrenamiento configurable mediante `train.py`.
* EvaluaciÃ³n y comparaciÃ³n con benchmarks tradicionales.
* Herramientas para generar visualizaciones de entrenamiento y evaluaciÃ³n.

## LÃ³gica interna

* **`portfolio_env.py`**: define la dinÃ¡mica de la cartera, las recompensas y los costes de transacciÃ³n.
* **`observation_builder.py`**: crea las observaciones que recibe el agente a partir de precios, indicadores y datos macro.
* **`metrics.py`**: calcula retorno final, ratio de Sharpe, drawdown y otras mÃ©tricas de rendimiento.
* **`benchmarks.py`**: implementa las estrategias clÃ¡sicas contra las que se evalÃºa el agente.
* **`train.py`**: orquesta el entrenamiento y la validaciÃ³n del modelo.
